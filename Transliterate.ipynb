{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fc2313c",
   "metadata": {},
   "source": [
    "### Import Basic Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f8b076d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os \n",
    "import io\n",
    "import time\n",
    "\n",
    "import string\n",
    "\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904bce0a",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5a219642",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse('C://Users//rochitranjan//Documents//NEWS2018_DATASET_04//NEWS2018_DATASET_04//NEWS2018_M-EnHi_trn.xml')\n",
    "\n",
    "elemList = []\n",
    "\n",
    "for elem in tree.iter():\n",
    "    elemList.append(elem.tag)\n",
    "\n",
    "root = tree.getroot()\n",
    "\n",
    "trans = {}\n",
    "for ele in root.findall('Name'):\n",
    "    w = ele.find('SourceName').text\n",
    "    w = w.strip(string.punctuation)\n",
    "    w = w.strip().split(' ')\n",
    "    tgt = ele.find('TargetName').text.split(' ')\n",
    "    if len(w) == len(tgt):\n",
    "        for ix in range(len(w)):\n",
    "            trans[w[ix]] = tgt[ix] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a98470e",
   "metadata": {},
   "source": [
    "### Preprocessing the input and output sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a1618171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "  # adding a start and an end token to the sentence\n",
    "  # so that the model know when to start and stop predicting.\n",
    "  w = list(w)\n",
    "  w = ['<sos>'] + w + ['<eos>']\n",
    "  return w\n",
    "\n",
    "input_sentences = [list(keys) for keys in trans.keys()]\n",
    "output_sentences = [list(values) for values in trans.values()]\n",
    "\n",
    "ip_sent = []\n",
    "for ip in input_sentences:\n",
    "    ip_sent.append(['<sos>'] + ip + ['<eos>']) #append the start and end tags to the tokenised sentences\n",
    "                                                          #each tokenied sentence is stored as a list in output sentences\n",
    "\n",
    "\n",
    "op_sent = []\n",
    "for op in output_sentences:\n",
    "    op_sent.append(['<sos>'] + op + ['<eos>']) #append the start and end tags to the tokenised sentences\n",
    "                                                          #each tokenied sentence is stored as a list in output sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b5a2a0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split words into characters and convert words into character id sequences.\n",
    "# Additionally, this function applies padding to the end of the sequence to standardise the sequence length\n",
    "def tokenize(lang, pad): \n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "  \n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "  \n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6483ee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function takes the list of input and target sequence and \n",
    "# returns the input tensor and output tensor for all the input and output sequence\n",
    "def load_dataset(inp_lang, targ_lang):\n",
    "  # creating cleaned input, output pairs\n",
    "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang, 'post')\n",
    "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang, 'post')\n",
    "\n",
    "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f1a079fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the load_dataset function to get input and target sequence in tensor form\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(ip_sent, op_sent)\n",
    "\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
    "\n",
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2,random_state=7)\n",
    "\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8aa5d8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUFFER_SIZE stores the number of training points\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "\n",
    "# BATCH_SIZE is set to 64. Training and gradient descent happens in batches of 32\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# the number of batches in one epoch (also, the number of steps during training, when we go batch by batch)\n",
    "steps_per_epoch = BUFFER_SIZE//BATCH_SIZE\n",
    "\n",
    "# the length of the embedded vector\n",
    "embedding_dim = 128\n",
    "\n",
    "# no of GRUs\n",
    "units = 512 \n",
    "\n",
    "# getting the size of the input and output vocabularies.\n",
    "vocab_inp_size = len(inp_lang.word_index)+1 \n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "# now, we shuffle the dataset and split it into batches of 32\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True) # the remainder after splitting by 32 are dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5283504a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([32, 24]), TensorShape([32, 24]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to understand the shape of an input batch\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aae1c6",
   "metadata": {},
   "source": [
    "### Encoder - Decoder using Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2dae178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz # set batch size\n",
    "    self.enc_units = enc_units # set the number of GRU units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) # set the embedding layer using the input's vocabulary size and the embedding dimension (which is set to 256). This learnt before feeding\n",
    "                                                                          # as an input to the GRU.\n",
    "\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True, # Return output of the GRU\n",
    "                                   return_state=True, # Return hidden state of the GRU\n",
    "                                   recurrent_initializer='glorot_uniform') # define the GRU layer\n",
    "\n",
    "  def call(self, x, hidden): # this function is invoked when the function encoder is called with an input and an initialised hidden layer\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden) # pass input x into the GRU layer and initialise the hidden state using initialize_hidden_state \n",
    "    return output, state # function returns the encoder output and the hidden state. In a traditional architecture of Seq2Seq modelling, output of Encoder is not used\n",
    "                         # rather hidden state of the encoder output is feed into decoder\n",
    "\n",
    "\n",
    "  def initialize_hidden_state(self): #intialise hidden layer to all zeroes (for determining the shape)\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e5890cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (32, 24, 512)\n",
      "Encoder Hidden state shape: (batch size, units) (32, 512)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE) # create an Encoder class object\n",
    "\n",
    "# sample input to get a sense of the shapes.\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7b1896",
   "metadata": {},
   "source": [
    "`The Below class gives an implementation of Bahdanau Additive Attention layer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "15076318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a class defined for the attention layer\n",
    "# returns attention weights and context vector.\n",
    "\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units) # fully-connected dense layer-1\n",
    "    self.W2 = tf.keras.layers.Dense(units) # fully-connected dense layer-2\n",
    "    self.V = tf.keras.layers.Dense(1) # fully-connected dense layer-3\n",
    "\n",
    "  def call(self, query, values):\n",
    "    # query hidden state shape == (batch_size, hidden size)\n",
    "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # values shape == (batch_size, max_len, hidden size)\n",
    "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "    query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "256e0be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape (context vector): (batch size, units) (32, 512)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (32, 24, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(20) # create an attention layer object\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output) # pass sample encoder output and hidden layer to get a sense of the shape of the output of the attention layer.\n",
    "\n",
    "print(\"Attention result shape (context vector): (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "09e7e89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz # batch_size which is defined as 32\n",
    "    self.dec_units = dec_units # the number of decoder GRU units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) # defining an embedding layer for the target language output. \n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform') # GRU layer\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # used for attention\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output) # getting the context vector and the attention weights from the attention layer\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x) # creating an embedding layer for the target output\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    " \n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "    \n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output) # pass the output through the dense layer\n",
    "\n",
    "    return x, state, attention_weights # return decoder output, decoder state and attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0ace9597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (32, 84)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "852ae10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none') #Loss function is categorical crossentropy which is the loss function used for typical classification problem.\n",
    "                                        # The reason we used from_logits = True is because the output of the decoder given by FC layer is in the form of logits\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0)) # the mask is used to ensure that the zero paddings which we have used is not using in modelling/training.\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a30be16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './tutorial_checkpoint_nmt'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029b8915",
   "metadata": {},
   "source": [
    "### Training and Evaluation of the Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "75c0fe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "# the train_step function defines one step of the training process. So, we have to repeat it for all the training items.\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden) # encoder has a inp(input) which is tensor for all of the elements of the\n",
    "                                                      # input language\n",
    "\n",
    "    dec_hidden = enc_hidden # decoder hidden value is the encoder_hidden value\n",
    "\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<sos>']] * BATCH_SIZE, 1) # the first value of the decoder input is set as the <SOS> string\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      # passing enc_hidden and enc_output to the decoder\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output) # In case of a decoder we give only one value out of the tensor batch_size and\n",
    "                                                               # expect only one output from decoder which is different from encoder\n",
    "                                                               # because entire tensor batch_size is feed into the encoder all at once.\n",
    "\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables # encoder.trainable_variables = weights of encoder\n",
    "                                                                        # decorder.trainable_variables = weights of decoder\n",
    "\n",
    "  gradients = tape.gradient(loss, variables) \n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables)) # doing gradient descent\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "96f9a0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.0514\n",
      "Epoch 1 Batch 100 Loss 0.0491\n",
      "Epoch 1 Batch 200 Loss 0.2025\n",
      "Epoch 1 Batch 300 Loss 0.0798\n",
      "Epoch 1 Loss 0.0899\n",
      "Time taken for 1 epoch 169.95730328559875 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.0430\n",
      "Epoch 2 Batch 100 Loss 0.0602\n",
      "Epoch 2 Batch 200 Loss 0.0512\n",
      "Epoch 2 Batch 300 Loss 0.0415\n",
      "Epoch 2 Loss 0.0598\n",
      "Time taken for 1 epoch 169.66071248054504 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.0359\n",
      "Epoch 3 Batch 100 Loss 0.0600\n",
      "Epoch 3 Batch 200 Loss 0.0498\n",
      "Epoch 3 Batch 300 Loss 0.0660\n",
      "Epoch 3 Loss 0.0462\n",
      "Time taken for 1 epoch 168.73134779930115 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.0341\n",
      "Epoch 4 Batch 100 Loss 0.0315\n",
      "Epoch 4 Batch 200 Loss 0.0439\n",
      "Epoch 4 Batch 300 Loss 0.0259\n",
      "Epoch 4 Loss 0.0376\n",
      "Time taken for 1 epoch 168.80465507507324 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.0253\n",
      "Epoch 5 Batch 100 Loss 0.0341\n",
      "Epoch 5 Batch 200 Loss 0.0343\n",
      "Epoch 5 Batch 300 Loss 0.0287\n",
      "Epoch 5 Loss 0.0336\n",
      "Time taken for 1 epoch 168.33061981201172 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.0277\n",
      "Epoch 6 Batch 100 Loss 0.0234\n",
      "Epoch 6 Batch 200 Loss 0.0398\n",
      "Epoch 6 Batch 300 Loss 0.0417\n",
      "Epoch 6 Loss 0.0372\n",
      "Time taken for 1 epoch 168.88288950920105 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.0294\n",
      "Epoch 7 Batch 100 Loss 0.0303\n",
      "Epoch 7 Batch 200 Loss 0.0652\n",
      "Epoch 7 Batch 300 Loss 0.0490\n",
      "Epoch 7 Loss 0.0485\n",
      "Time taken for 1 epoch 571.9252898693085 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.1216\n",
      "Epoch 8 Batch 100 Loss 0.0319\n",
      "Epoch 8 Batch 200 Loss 0.0246\n",
      "Epoch 8 Batch 300 Loss 0.0435\n",
      "Epoch 8 Loss 0.0415\n",
      "Time taken for 1 epoch 169.17676973342896 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0315\n",
      "Epoch 9 Batch 100 Loss 0.0337\n",
      "Epoch 9 Batch 200 Loss 0.0287\n",
      "Epoch 9 Batch 300 Loss 0.0207\n",
      "Epoch 9 Loss 0.0284\n",
      "Time taken for 1 epoch 168.19593930244446 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0281\n",
      "Epoch 10 Batch 100 Loss 0.0237\n",
      "Epoch 10 Batch 200 Loss 0.0284\n",
      "Epoch 10 Batch 300 Loss 0.0250\n",
      "Epoch 10 Loss 0.0243\n",
      "Time taken for 1 epoch 168.16178822517395 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.0286\n",
      "Epoch 11 Batch 100 Loss 0.0222\n",
      "Epoch 11 Batch 200 Loss 0.0195\n",
      "Epoch 11 Batch 300 Loss 0.0319\n",
      "Epoch 11 Loss 0.0255\n",
      "Time taken for 1 epoch 167.92991137504578 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.0876\n",
      "Epoch 12 Batch 100 Loss 0.0453\n",
      "Epoch 12 Batch 200 Loss 0.1576\n",
      "Epoch 12 Batch 300 Loss 0.0215\n",
      "Epoch 12 Loss 0.0346\n",
      "Time taken for 1 epoch 168.03704380989075 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.0307\n",
      "Epoch 13 Batch 100 Loss 0.0567\n",
      "Epoch 13 Batch 200 Loss 0.0585\n",
      "Epoch 13 Batch 300 Loss 0.0540\n",
      "Epoch 13 Loss 0.0539\n",
      "Time taken for 1 epoch 167.62025475502014 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.0282\n",
      "Epoch 14 Batch 100 Loss 0.0279\n",
      "Epoch 14 Batch 200 Loss 0.0356\n",
      "Epoch 14 Batch 300 Loss 0.0420\n",
      "Epoch 14 Loss 0.0335\n",
      "Time taken for 1 epoch 167.7111532688141 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.0286\n",
      "Epoch 15 Batch 100 Loss 0.0202\n",
      "Epoch 15 Batch 200 Loss 0.0178\n",
      "Epoch 15 Batch 300 Loss 0.0234\n",
      "Epoch 15 Loss 0.0240\n",
      "Time taken for 1 epoch 167.67079758644104 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.0139\n",
      "Epoch 16 Batch 100 Loss 0.0109\n",
      "Epoch 16 Batch 200 Loss 0.0518\n",
      "Epoch 16 Batch 300 Loss 0.0336\n",
      "Epoch 16 Loss 0.0224\n",
      "Time taken for 1 epoch 167.83575224876404 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.0185\n",
      "Epoch 17 Batch 100 Loss 0.0216\n",
      "Epoch 17 Batch 200 Loss 0.0193\n",
      "Epoch 17 Batch 300 Loss 0.0364\n",
      "Epoch 17 Loss 0.0238\n",
      "Time taken for 1 epoch 167.03630232810974 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.0244\n",
      "Epoch 18 Batch 100 Loss 0.0179\n",
      "Epoch 18 Batch 200 Loss 0.0780\n",
      "Epoch 18 Batch 300 Loss 0.0527\n",
      "Epoch 18 Loss 0.0340\n",
      "Time taken for 1 epoch 170.02741193771362 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.0376\n",
      "Epoch 19 Batch 100 Loss 0.0274\n",
      "Epoch 19 Batch 200 Loss 0.0308\n",
      "Epoch 19 Batch 300 Loss 0.0309\n",
      "Epoch 19 Loss 0.0327\n",
      "Time taken for 1 epoch 167.98289322853088 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.0140\n",
      "Epoch 20 Batch 100 Loss 0.0204\n",
      "Epoch 20 Batch 200 Loss 0.0218\n",
      "Epoch 20 Batch 300 Loss 0.0148\n",
      "Epoch 20 Loss 0.0180\n",
      "Time taken for 1 epoch 168.1045424938202 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.0089\n",
      "Epoch 21 Batch 100 Loss 0.0147\n",
      "Epoch 21 Batch 200 Loss 0.0181\n",
      "Epoch 21 Batch 300 Loss 0.0181\n",
      "Epoch 21 Loss 0.0126\n",
      "Time taken for 1 epoch 167.72927141189575 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.0032\n",
      "Epoch 22 Batch 100 Loss 0.0065\n",
      "Epoch 22 Batch 200 Loss 0.0076\n",
      "Epoch 22 Batch 300 Loss 0.0209\n",
      "Epoch 22 Loss 0.0185\n",
      "Time taken for 1 epoch 168.13544511795044 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.0817\n",
      "Epoch 23 Batch 100 Loss 0.0686\n",
      "Epoch 23 Batch 200 Loss 0.0716\n",
      "Epoch 23 Batch 300 Loss 0.0482\n",
      "Epoch 23 Loss 0.0608\n",
      "Time taken for 1 epoch 167.99102973937988 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.0132\n",
      "Epoch 24 Batch 100 Loss 0.0191\n",
      "Epoch 24 Batch 200 Loss 0.0146\n",
      "Epoch 24 Batch 300 Loss 0.0216\n",
      "Epoch 24 Loss 0.0202\n",
      "Time taken for 1 epoch 168.2015790939331 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.0078\n",
      "Epoch 25 Batch 100 Loss 0.0149\n",
      "Epoch 25 Batch 200 Loss 0.0089\n",
      "Epoch 25 Batch 300 Loss 0.0079\n",
      "Epoch 25 Loss 0.0126\n",
      "Time taken for 1 epoch 167.93684673309326 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.0040\n",
      "Epoch 26 Batch 100 Loss 0.0126\n",
      "Epoch 26 Batch 200 Loss 0.0075\n",
      "Epoch 26 Batch 300 Loss 0.0103\n",
      "Epoch 26 Loss 0.0092\n",
      "Time taken for 1 epoch 167.93851256370544 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.0037\n",
      "Epoch 27 Batch 100 Loss 0.0031\n",
      "Epoch 27 Batch 200 Loss 0.0070\n",
      "Epoch 27 Batch 300 Loss 0.0170\n",
      "Epoch 27 Loss 0.0073\n",
      "Time taken for 1 epoch 168.55487298965454 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.0017\n",
      "Epoch 28 Batch 100 Loss 0.0076\n",
      "Epoch 28 Batch 200 Loss 0.0062\n",
      "Epoch 28 Batch 300 Loss 0.1686\n",
      "Epoch 28 Loss 0.0291\n",
      "Time taken for 1 epoch 168.04462575912476 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.0610\n",
      "Epoch 29 Batch 100 Loss 0.0419\n",
      "Epoch 29 Batch 200 Loss 0.0230\n",
      "Epoch 29 Batch 300 Loss 0.0312\n",
      "Epoch 29 Loss 0.0444\n",
      "Time taken for 1 epoch 168.04927325248718 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.0073\n",
      "Epoch 30 Batch 100 Loss 0.0076\n",
      "Epoch 30 Batch 200 Loss 0.0165\n",
      "Epoch 30 Batch 300 Loss 0.0304\n",
      "Epoch 30 Loss 0.0191\n",
      "Time taken for 1 epoch 168.72861456871033 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train = True\n",
    "EPOCHS = 30\n",
    "if train :\n",
    "  for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    # for each of the data point in the batch_size we put it thru the train_step\n",
    "      batch_loss = train_step(inp, targ, enc_hidden)\n",
    "      total_loss += batch_loss\n",
    "\n",
    "      if batch % 100 == 0:\n",
    "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                    batch,\n",
    "                                                    batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "14cccbed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1e9d83589a0>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "decb6a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(word):\n",
    "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "  word = preprocess_sentence(word)\n",
    "\n",
    "  inputs = [inp_lang.word_index[i] for i in word]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  hidden = [tf.zeros((1, units))]\n",
    "  enc_out, enc_hidden = encoder(inputs, hidden) # for encoder a zero matrix is provided as initial state for the first time, while initialising it.\n",
    "\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_input = tf.expand_dims([targ_lang.word_index['<sos>']], 0) # define <SOS> as the first input to the decoder\n",
    "\n",
    "  for t in range(max_length_targ):\n",
    "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                         dec_hidden,\n",
    "                                                         enc_out)\n",
    "    \n",
    "    # pass the encoder output, decoder hidden state(which is initialised to encoder hidden state for the first time and decoder input to the decoder)\n",
    "    # make a prediction and obtain decoder hidden states\n",
    "\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy() # Use the word_id which has the maximum probability of occurence as the prediction for each of the decoder output\n",
    "\n",
    "    if targ_lang.index_word[predicted_id] != '<eos>':\n",
    "      result += targ_lang.index_word[predicted_id] + ' '\n",
    "    else:\n",
    "      return result, word#, attention_plot\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, word#, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "472cf204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transliterate(word):\n",
    "  result, word = evaluate(word)\n",
    "#  print('Input: %s' % (word))\n",
    "  print('Predicted transliteration: {}'.format(result.replace(' ', '')))\n",
    "  return result.replace(' ', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46342cef",
   "metadata": {},
   "source": [
    "### Sample Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "00e84556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type your input word: algorithm\n",
      "Predicted transliteration: एल्गोरिथम\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'एल्गोरिथम'"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = input('Type your input word: ')\n",
    "transliterate(list(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "859ba5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type your input word: abhigyaan\n",
      "Predicted transliteration: अभिग्ञान\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'अभिग्ञान'"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = input('Type your input word: ')\n",
    "transliterate(list(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "568da744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type your input word: namaste\n",
      "Predicted transliteration: नमस्ते\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'नमस्ते'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = input('Type your input word: ')\n",
    "transliterate(list(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "8c1d76c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type your input word: nonsense\n",
      "Predicted transliteration: नॉनसेंस\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'नॉनसेंस'"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = input('Type your input word: ')\n",
    "transliterate(list(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "6b766569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type your input word: idiot\n",
      "Predicted transliteration: इडियोत\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'इडियोत'"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = input('Type your input word: ')\n",
    "transliterate(list(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "49746fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type your input word: important\n",
      "Predicted transliteration: इम्पोर्टेंत\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'इम्पोर्टेंत'"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = input('Type your input word: ')\n",
    "transliterate(list(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "bffaec3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type your input word: pneumonia\n",
      "Predicted transliteration: प्नूमोनिया\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'प्नूमोनिया'"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = input('Type your input word: ')\n",
    "transliterate(list(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "196a603c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type your input word: horse\n",
      "Predicted transliteration: हॉर्स\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'हॉर्स'"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = input('Type your input word: ')\n",
    "transliterate(list(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "ff21611f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type your input word: generous\n",
      "Predicted transliteration: जनरोउस\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'जनरोउस'"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = input('Type your input word: ')\n",
    "transliterate(list(word))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
